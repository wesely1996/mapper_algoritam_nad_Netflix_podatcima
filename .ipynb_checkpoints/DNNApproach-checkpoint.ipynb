{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_data(data_path):\n",
    "    r\"\"\"Reads user data and refine it.\"\"\"\n",
    "    df_raw = pd.read_csv(data_path, header=None, names=[\"User\", \"Rating\", \"Date\"], usecols=[0, 1, 2])\n",
    "\n",
    "    tmp_movies = df_raw[df_raw[\"Rating\"].isna()][\"User\"].reset_index()\n",
    "    movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "\n",
    "    shifted_movie_indices = deque(movie_indices)\n",
    "    shifted_movie_indices.rotate(-1)\n",
    "\n",
    "    user_data = []\n",
    "    for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "        if df_id_1<df_id_2:\n",
    "            tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "        else:\n",
    "            tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
    "            \n",
    "        tmp_df[\"Movie\"] = movie_id\n",
    "        user_data.append(tmp_df)\n",
    "\n",
    "    df = pd.concat(user_data)\n",
    "    print(f\"Shape of raw User-Ratings: {df.shape}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, movie_rating_threshold=10000, user_rating_threshold=200):\n",
    "    r\"\"\"Discard outliers from data in both movies and users.\"\"\"\n",
    "    filter_movies = (df[\"Movie\"].value_counts() > movie_rating_threshold)\n",
    "    filter_movies = filter_movies[filter_movies].index.tolist()\n",
    "\n",
    "    filter_users = (df[\"User\"].value_counts() > user_rating_threshold)\n",
    "    filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "    df_filtered = df[(df['Movie'].isin(filter_movies)) & (df['User'].isin(filter_users))]\n",
    "\n",
    "    print(f\"Shape of filtered User-Ratings: {df_filtered.shape}\")\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(df_filtered, n=100000):\n",
    "    r\"\"\"Splits dataframe into training and test.\"\"\"\n",
    "    df_filtered = df_filtered.drop('Date', axis=1).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # splits data into training and test\n",
    "    df_train = df_filtered[:-n]\n",
    "    df_test = df_filtered[-n:]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: not used for DNN approach\n",
    "def make_data_sparse(df):\n",
    "    df_p = df.pivot_table(index=\"User\", columns=\"Movie\", values=\"Rating\", fill_value=0)\n",
    "\n",
    "    print(f\"Shape of User-Movie: {df_p.shape}\")\n",
    "\n",
    "    return df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_movie_titles(data_path):\n",
    "    r\"\"\"Reads movie titles and returns dataframe.\"\"\"\n",
    "    movie_titles = pd.read_csv(data_path, \n",
    "                               encoding=\"ISO-8859-1\", \n",
    "                               header=None, \n",
    "                               names=[\"Id\", \"Year\", \"Name\"]).set_index(\"Id\")\n",
    "\n",
    "    print(f\"Shape of Movie-Titles: {movie_titles.shape}\")\n",
    "\n",
    "    return movie_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Movie-Titles: (17770, 2)\n",
      "Shape of raw User-Ratings: (24053764, 4)\n",
      "Shape of filtered User-Ratings: (4178032, 4)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "data_path = os.path.join(data_dir, \"combined_data_1.txt\")\n",
    "# data_path = os.path.join(data_dir, \"combined_data_all.txt\")\n",
    "\n",
    "movie_titles = fetch_movie_titles(os.path.join(data_dir, \"movie_titles.csv\"))\n",
    "\n",
    "df_raw = fetch_user_data(data_path)\n",
    "df_filtered = filter_data(df_raw)\n",
    "df_train, df_test = create_train_test_split(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw number of users: 470758\n",
      "Raw number of movies: 4499\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw number of users: {len(df_raw['User'].unique())}\")\n",
    "print(f\"Raw number of movies: {len(df_raw['Movie'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 20828\n",
      "Number of movies: 491\n"
     ]
    }
   ],
   "source": [
    "# will map all IDs to [0, N]\n",
    "user_id_mapping = {unique_id: i for i, unique_id in enumerate(df_filtered[\"User\"].unique())}\n",
    "user_id_unmapping = {i: unique_id for i, unique_id in enumerate(df_filtered[\"User\"].unique())}\n",
    "movie_id_mapping = {unique_id: i for i, unique_id in enumerate(df_filtered[\"Movie\"].unique())}\n",
    "movie_id_unmapping = {i: unique_id for i, unique_id in enumerate(df_filtered[\"Movie\"].unique())}\n",
    "\n",
    "train_user_data = df_train[\"User\"].map(user_id_mapping)\n",
    "train_movie_data = df_train[\"Movie\"].map(movie_id_mapping)\n",
    "\n",
    "# same mapping ised for both training and test data\n",
    "test_user_data = df_test[\"User\"].map(user_id_mapping)\n",
    "test_movie_data = df_test[\"Movie\"].map(movie_id_mapping)\n",
    "\n",
    "n_users = len(user_id_mapping)\n",
    "print(f\"Number of users: {n_users}\")\n",
    "\n",
    "n_movies = len(movie_id_mapping)\n",
    "print(f\"Number of movies: {n_movies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_size = 12\n",
    "movie_embedding_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input = Input(shape=[1], name=\"user\")\n",
    "movie_id_input = Input(shape=[1], name=\"movie\")\n",
    "\n",
    "# Create embedding layers for users and movies\n",
    "user_embedding = Embedding(output_dim=user_embedding_size, \n",
    "                           input_dim=n_users,\n",
    "                           input_length=1, \n",
    "                           name=\"user_embedding\")(user_id_input)\n",
    "movie_embedding = Embedding(output_dim=movie_embedding_size, \n",
    "                            input_dim=n_movies,\n",
    "                            input_length=1, \n",
    "                            name=\"item_embedding\")(movie_id_input)\n",
    "\n",
    "# reshaping because embedding's output will have redundant dimension\n",
    "user_vector = Reshape([user_embedding_size])(user_embedding)\n",
    "movie_vector = Reshape([movie_embedding_size])(movie_embedding)\n",
    "\n",
    "# input to model will be embedding of both movie and user\n",
    "concat_input = Concatenate()([user_vector, movie_vector])\n",
    "\n",
    "# model is shallow MLP\n",
    "dense1 = Dense(256)(concat_input)\n",
    "dropout1 = Dropout(0.5)(dense1, training=True)\n",
    "dense2 = Dense(32)(dropout1)\n",
    "y = Dense(1)(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7169/7169 [==============================] - 75s 9ms/step - loss: 1.1187 - val_loss: 0.8259\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "model = Model(inputs=[user_id_input, movie_id_input], outputs=y)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# train model\n",
    "train_history = model.fit(x=[train_user_data, train_movie_data],\n",
    "                          y=df_train[\"Rating\"],\n",
    "                          batch_size=512, \n",
    "                          epochs=1,\n",
    "                          validation_split=0.1,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_user_data, test_movie_data])\n",
    "y_true = df_test[\"Rating\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test set: 0.8305469153993164\n",
      "RMSE on test set: 0.9113434673048995\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_pred=y_pred, y_true=y_true)\n",
    "print(f\"MSE on test set: {mse}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE on test set: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_forward_passes = 5\n",
    "recommend_threshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend_preds shape: (5, 100000)\n"
     ]
    }
   ],
   "source": [
    "recommend_preds = []\n",
    "for i in range(n_forward_passes):\n",
    "    y_pred = model.predict([test_user_data, test_movie_data])\n",
    "\n",
    "    recommend_pred = np.squeeze(y_pred)\n",
    "    min_pred = np.min(recommend_pred)\n",
    "    max_pred = np.max(recommend_pred)\n",
    "    recommend_pred = (recommend_pred - min_pred) / (max_pred - min_pred)\n",
    "    recommend_preds.append(recommend_pred)\n",
    "\n",
    "recommend_preds = np.array(recommend_preds)\n",
    "print(f\"recommend_preds shape: {recommend_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend_true shape: (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "recommend_true = np.squeeze(y_true) / 5\n",
    "recommend_true = recommend_true >= recommend_threshold\n",
    "recommend_true = np.intp(recommend_true).astype(np.float32)\n",
    "recommend_true = np.expand_dims(recommend_true, axis=1)\n",
    "print(f\"recommend_true shape: {recommend_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating statistics\n",
    "recommend_std = np.std(recommend_preds, axis=0).reshape(-1, 1)\n",
    "recommend_mean = np.mean(recommend_preds, axis=0).reshape(-1, 1)\n",
    "recommend_modus = (np.mean(recommend_preds >= recommend_threshold, axis=0) >= recommend_threshold).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring equality of shapes\n",
    "assert recommend_std.shape == recommend_mean.shape\n",
    "assert recommend_std.shape == recommend_modus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate shape: (100000, 32)\n"
     ]
    }
   ],
   "source": [
    "# creating submodel for gathering intermediate activations\n",
    "dense_1_model = Model(inputs=model.input, outputs=model.get_layer(\"dense_1\").output)\n",
    "intermediate_preds = dense_1_model.predict([test_user_data, test_movie_data])\n",
    "print(f\"Intermediate shape: {intermediate_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection shape: (100000, 2)\n"
     ]
    }
   ],
   "source": [
    "projection_preds = np.c_[recommend_std, np.abs(recommend_true - recommend_mean)]\n",
    "print(f\"Projection shape: {projection_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeplerMapper(verbose=2)\n",
      "Mapping on data shaped (100000, 32) using lens shaped (100000, 2)\n",
      "\n",
      "Minimal points in hypercube before clustering: 2\n",
      "Creating 100 hypercubes.\n",
      "   > Found 2 clusters in hypercube 0.\n",
      "   > Found 2 clusters in hypercube 1.\n",
      "   > Found 2 clusters in hypercube 2.\n",
      "   > Found 2 clusters in hypercube 3.\n",
      "   > Found 2 clusters in hypercube 4.\n",
      "   > Found 2 clusters in hypercube 5.\n",
      "   > Found 2 clusters in hypercube 6.\n",
      "   > Found 2 clusters in hypercube 7.\n",
      "   > Found 2 clusters in hypercube 8.\n",
      "   > Found 2 clusters in hypercube 9.\n",
      "   > Found 2 clusters in hypercube 10.\n",
      "   > Found 2 clusters in hypercube 11.\n",
      "   > Found 2 clusters in hypercube 12.\n",
      "   > Found 2 clusters in hypercube 13.\n",
      "   > Found 2 clusters in hypercube 14.\n",
      "   > Found 2 clusters in hypercube 15.\n",
      "   > Found 2 clusters in hypercube 16.\n",
      "   > Found 2 clusters in hypercube 17.\n",
      "Cube_18 is empty.\n",
      "\n",
      "   > Found 2 clusters in hypercube 19.\n",
      "   > Found 2 clusters in hypercube 20.\n",
      "   > Found 2 clusters in hypercube 21.\n",
      "   > Found 2 clusters in hypercube 22.\n",
      "   > Found 2 clusters in hypercube 23.\n",
      "   > Found 2 clusters in hypercube 24.\n",
      "   > Found 2 clusters in hypercube 25.\n",
      "   > Found 2 clusters in hypercube 26.\n",
      "   > Found 2 clusters in hypercube 27.\n",
      "   > Found 2 clusters in hypercube 28.\n",
      "   > Found 2 clusters in hypercube 29.\n",
      "   > Found 2 clusters in hypercube 30.\n",
      "   > Found 2 clusters in hypercube 31.\n",
      "   > Found 2 clusters in hypercube 32.\n",
      "   > Found 2 clusters in hypercube 33.\n",
      "   > Found 2 clusters in hypercube 34.\n",
      "   > Found 2 clusters in hypercube 35.\n",
      "   > Found 2 clusters in hypercube 36.\n",
      "   > Found 2 clusters in hypercube 37.\n",
      "   > Found 2 clusters in hypercube 38.\n",
      "   > Found 2 clusters in hypercube 39.\n",
      "   > Found 2 clusters in hypercube 40.\n",
      "   > Found 2 clusters in hypercube 41.\n",
      "   > Found 2 clusters in hypercube 42.\n",
      "   > Found 2 clusters in hypercube 43.\n",
      "   > Found 2 clusters in hypercube 44.\n",
      "   > Found 2 clusters in hypercube 45.\n",
      "   > Found 2 clusters in hypercube 46.\n",
      "   > Found 2 clusters in hypercube 47.\n",
      "   > Found 2 clusters in hypercube 48.\n",
      "   > Found 2 clusters in hypercube 49.\n",
      "   > Found 2 clusters in hypercube 50.\n",
      "   > Found 2 clusters in hypercube 51.\n",
      "   > Found 2 clusters in hypercube 52.\n",
      "   > Found 2 clusters in hypercube 53.\n",
      "   > Found 2 clusters in hypercube 54.\n",
      "   > Found 2 clusters in hypercube 55.\n",
      "   > Found 2 clusters in hypercube 56.\n",
      "   > Found 2 clusters in hypercube 57.\n",
      "   > Found 2 clusters in hypercube 58.\n",
      "   > Found 2 clusters in hypercube 59.\n",
      "   > Found 2 clusters in hypercube 60.\n",
      "   > Found 2 clusters in hypercube 61.\n",
      "   > Found 2 clusters in hypercube 62.\n",
      "   > Found 2 clusters in hypercube 63.\n",
      "   > Found 2 clusters in hypercube 64.\n",
      "   > Found 2 clusters in hypercube 65.\n",
      "   > Found 2 clusters in hypercube 66.\n",
      "Cube_67 is empty.\n",
      "\n",
      "Cube_68 is empty.\n",
      "\n",
      "Cube_69 is empty.\n",
      "\n",
      "   > Found 2 clusters in hypercube 70.\n",
      "   > Found 2 clusters in hypercube 71.\n",
      "\n",
      "Created 269 edges and 136 nodes in 0:03:01.839563.\n"
     ]
    }
   ],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "G = mapper.map(projection_preds,\n",
    "               intermediate_preds,\n",
    "               clusterer=AgglomerativeClustering(n_clusters=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooltip_s = []\n",
    "for movie_id, user_id in zip(test_movie_data, test_user_data):\n",
    "    movie_name = str(movie_titles.iloc[movie_id_unmapping.get(movie_id)]['Name'])\n",
    "    info = \"\"\"<div style=\"width:200px;\n",
    "                             height:30px;\n",
    "                             overflow-x:hidden;\n",
    "                             overflow-y:auto;\n",
    "                             float:left;\n",
    "                             position: relative;\">\n",
    "                 <div style=\"position: relative; top:0; right:0; font-size:10px\">Movie Name: %s</div>\n",
    "                 <div style=\"position: relative; top: 0; right: 0; font-size:10px\">User ID: %s</div>\n",
    "                 </div>\"\"\" % ((movie_name, user_id))\n",
    "    tooltip_s.append(info)\n",
    "tooltip_s = np.array(tooltip_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote visualization to: visualization.html\n"
     ]
    }
   ],
   "source": [
    "_ = mapper.visualize(G,\n",
    "                     lens=projection_preds,\n",
    "                     lens_names=[\"STD\", \"L1\"],\n",
    "                     custom_tooltips=tooltip_s,\n",
    "                     title=\"STD vs. L1\",\n",
    "                     path_html=\"visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
